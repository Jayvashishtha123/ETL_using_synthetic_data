{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cbd957",
   "metadata": {},
   "source": [
    "Part 1: Synthetic Data Generation (Faker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation complete ✅\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import builtins   \n",
    "\n",
    "\n",
    "SEED = 42\n",
    "fake = Faker()\n",
    "Faker.seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "NUM_RECORDS = 100_000\n",
    "NUM_USERS = 75\n",
    "\n",
    "departments = [\"HR\", \"IT\", \"Finance\", \"Sales\", \"Operations\"]\n",
    "\n",
    "users = [\n",
    "    {\n",
    "        \"user_id\": i + 1,\n",
    "        \"user_name\": fake.name(),\n",
    "        \"department\": random.choice(departments)\n",
    "    }\n",
    "    for i in range(NUM_USERS)\n",
    "]\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "\n",
    "\n",
    "with open(\"employee_logs.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerow([\n",
    "        \"user_id\",\n",
    "        \"user_name\",\n",
    "        \"department\",\n",
    "        \"activity_date\",\n",
    "        \"expected_check_in\",\n",
    "        \"actual_check_in\",\n",
    "        \"actual_check_out\",\n",
    "        \"idle_hours\",\n",
    "        \"is_on_leave\"\n",
    "    ])\n",
    "\n",
    "    for _ in range(NUM_RECORDS):\n",
    "\n",
    "        user = random.choice(users)\n",
    "\n",
    "        date = start_date + timedelta(days=random.randint(0, 180))\n",
    "\n",
    "        expected_check_in = datetime(\n",
    "            date.year, date.month, date.day, 9, 0, 0\n",
    "        )\n",
    "\n",
    "        is_on_leave = random.random() < 0.1\n",
    "\n",
    "        if is_on_leave:\n",
    "            actual_check_in = \"\"\n",
    "            actual_check_out = \"\"\n",
    "            idle_hours = 0.0\n",
    "        else:\n",
    "            late_minutes = random.randint(0, 90)\n",
    "            actual_check_in = expected_check_in + timedelta(minutes=late_minutes)\n",
    "\n",
    "            work_hours = random.randint(7, 9)\n",
    "            actual_check_out = actual_check_in + timedelta(hours=work_hours)\n",
    "\n",
    "            idle_hours = builtins.round(random.uniform(0, 2), 2)\n",
    "\n",
    "        writer.writerow([\n",
    "            user[\"user_id\"],\n",
    "            user[\"user_name\"],\n",
    "            user[\"department\"],\n",
    "            date.strftime(\"%Y-%m-%d\"),\n",
    "            expected_check_in.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            actual_check_in.strftime(\"%Y-%m-%d %H:%M:%S\") if actual_check_in else \"\",\n",
    "            actual_check_out.strftime(\"%Y-%m-%d %H:%M:%S\") if actual_check_out else \"\",\n",
    "            idle_hours,\n",
    "            is_on_leave\n",
    "        ])\n",
    "\n",
    "print(\"Data generation complete ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186b19f",
   "metadata": {},
   "source": [
    "Part 2: Data Ingestion with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark started successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count: 100000\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- activity_date: date (nullable = true)\n",
      " |-- expected_check_in: timestamp (nullable = true)\n",
      " |-- actual_check_in: timestamp (nullable = true)\n",
      " |-- actual_check_out: timestamp (nullable = true)\n",
      " |-- idle_hours: double (nullable = true)\n",
      " |-- is_on_leave: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+-----------------+------------------+------------------+------------+------------------+\n",
      "|user_id|     user_name|department|avg_working_hours|    avg_late_hours|  total_idle_hours|total_leaves|late_arrival_count|\n",
      "+-------+--------------+----------+-----------------+------------------+------------------+------------+------------------+\n",
      "|     36|Jeffrey Chavez|        HR|6.484290271132376|0.7076555023923442|1173.6999999999996|          87|              1153|\n",
      "|     63|   Connor West|     Sales|6.447621998450814|0.7061192873741283|1207.1200000000003|          99|              1179|\n",
      "|     50|   Eric Carney|Operations|6.498134796238244|0.7099268547544413|           1204.38|          95|              1174|\n",
      "|     70| William Baker|        IT|6.500922953451041|0.7244248261102193|1200.8499999999995|          88|              1144|\n",
      "|     25|Matthew Foster|     Sales|6.581120491174214|0.6984139166027115|1201.8000000000002|          86|              1205|\n",
      "+-------+--------------+----------+-----------------+------------------+------------------+------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Employee_ETL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark started successfully\")\n",
    "df = spark.read.csv(\n",
    "    \"employee_logs.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"Initial count:\", df.count())\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"activity_date\",\n",
    "    F.to_date(\"activity_date\")\n",
    ").withColumn(\n",
    "    \"expected_check_in\",\n",
    "    F.to_timestamp(\"expected_check_in\")\n",
    ").withColumn(\n",
    "    \"actual_check_in\",\n",
    "    F.to_timestamp(\"actual_check_in\")\n",
    ").withColumn(\n",
    "    \"actual_check_out\",\n",
    "    F.to_timestamp(\"actual_check_out\")\n",
    ")\n",
    "\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"working_hours\",\n",
    "    F.when(\n",
    "        F.col(\"is_on_leave\") == False,\n",
    "        (\n",
    "            (F.unix_timestamp(\"actual_check_out\") -\n",
    "             F.unix_timestamp(\"actual_check_in\")) / 3600\n",
    "        ) - F.col(\"idle_hours\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"late_hours\",\n",
    "    F.when(\n",
    "        F.col(\"actual_check_in\") > F.col(\"expected_check_in\"),\n",
    "        (F.unix_timestamp(\"actual_check_in\") -\n",
    "         F.unix_timestamp(\"expected_check_in\")) / 3600\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "\n",
    "user_metrics = df.groupBy(\n",
    "    \"user_id\", \"user_name\", \"department\"\n",
    ").agg(\n",
    "    F.avg(\"working_hours\").alias(\"avg_working_hours\"),\n",
    "    F.avg(\"late_hours\").alias(\"avg_late_hours\"),\n",
    "    F.sum(\"idle_hours\").alias(\"total_idle_hours\"),\n",
    "    F.sum(F.when(F.col(\"is_on_leave\") == True, 1).otherwise(0)).alias(\"total_leaves\"),\n",
    "    F.sum(F.when(F.col(\"late_hours\") > 0, 1).otherwise(0)).alias(\"late_arrival_count\")\n",
    ")\n",
    "\n",
    "user_metrics.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0463fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to PostgreSQL successfully\n"
     ]
    }
   ],
   "source": [
    "jdbc_url = \"jdbc:postgresql://localhost:5432/etl_db\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"Jay@101252\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "user_metrics.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"employee_user_metrics\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(\"Data written to PostgreSQL successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark312_env (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
